\chapter{Background}

Before we proceed we shall first set the stage by discussing in depth the GHC's intermediate language and its
accessory set of optimisation transformations. We then discuss its occasional shortcomings to make the point that
inspecting GHC's intermediate results is necessary at times. 
Finally we summarize contemporary solutions to that problem and how they can be unsatisfactory.

\section{GHC: an optimising compiler}

GHC is an optimising compiler, meaning that it is not just expected to convert Haskell source to a semantically equivalent binary,
it also attempts to apply correctness-preserving transformations that reduce runtime costs. Given that a Haskell program is very
restricted when it comes to effects, such transformations are empowered to make large changes.

\subsection{Example: \mono{triangular}}
Consider for example the function that given a positive integer \mono{n}, returns the \mono{n}th triangular number (i.e. sum of all integers up to and including \mono{n}).

\begin{listing}[H]
\begin{minted}[linenos]{haskell}
triangular :: Int -> Int
triangular 0 = 0
triangular n = n + triangular (n-1)
\end{minted}
\caption{\mono{GHC -O0}: The triangular function in plain Haskell, left untouched by compiling with \mono{-O0}}
\end{listing}

Compiling this definition, disabling any optimisations using \mono{-O0}, will yield a very suboptimal binary. The main
reason for this are heap allocations. Namely, Haskell's non strictness semantics mean that values can also represent
future calculations. Because the size of these so-called \textit{thunks} is not known at compile time, almost all
values are heap allocated, which is usually more costly than allocating on the stack. We can represent the
unoptimized \mono{triangular} program as a C program with equivalent allocation behavior:

\begin{minted}[linenos]{c}
#include <stdio.h>
#include <stdlib.h>

long* alloc_long() { return (long*)malloc(sizeof(long)); }
long* triangular(const long* n_ptr) {
  long* ret = alloc_long();
  long n = *n_ptr;
  if (n==0) { *ret = 0; } else {
    long* inp_ptr = alloc_long();
    *inp_ptr = n-1 ;
    *ret = n + *triangular(inp_ptr);
  }
  return ret;
}
\end{minted}

If we do enable GHC's optimisation passes, we find that an auxiliary definition is created that operates
on \textit{unboxed integers} (\hs{type Int = I# Int#}), where a value of type \mono{Int#} 
is always strict. From this follows that the size is always 64 bits and thus stack allocations are possible.
This significantly reduces the number of heap allocations and improves performance. Aside from a number of simplification 
steps, most of the heavy lifting is done here by the \textit{Worker Wrapper binds} pass to yield the following optimised program:

\begin{listing}[H]
\begin{minted}[linenos]{haskell}
wtriangular :: Int# -> Int#
wtriangular ww = case ww of
  { 0 -> 0
    ds -> case wtriangular
                 (GHC.Prim.-# ds 1) of
      { ww -> GHC.Prim.+# ds ww
      }
  }

triangular :: Int -> Int
triangular w = case w of
  { I# ww -> case wtriangular ww of
      { ww -> GHC.Types.I# ww
      }
  }
\end{minted}
\caption{\mono{GHC -01}: The triangular function in Haskell, with reduced heap allocations. As produced by compiling with \mono{-O1}.}
\end{listing}

We can profile the result of each of the two Haskell snippets above to get an idea of the runtime impact, showing the
importance of this transformation pipeline.

\begin{table}[H]
\begin{tabular}{|l|l|}
\textbf{Snippet} & \textbf{Result of \mono{time}}\\
\hline \\
\mono{GHC -O0} & 0,10s user 0,03s system 99\% cpu 0,132 total \\
\mono{GHC -O1} & 0,01s user 0,01s system 98\% cpu 0,023 total \\
\end{tabular}
\caption{The runtime of each listing. We can see that the optimisation has increased runtime performance by a significant factor.}
\end{table}


\subsection{Splitting responsibility}

We discussed how GHC is able to make some pretty drastic changes to the source to increase performance.
Such a transform would conceivably be of type  \hs{HsModule -> HsModule}. However, Haskell is a very rich language
with many language features, and its AST has dozens of constructors to represent all these possibilities.
Developing aforementioned transformations on the source would be a particularly strenuous task, not to mention verifying 
their correctness-preserving property (even empirically). Luckily, upon further inspection it becomes apparent that many
language features are syntax sugar for - or similarly simple to convert to - other constructors.

This idea is in fact exploitable to such a great extent, that all Haskell source programs can be projected onto a much
smaller, but semantically equivalent \textit{core}-language. This leads to a setup with 3 steps with very distinct responsibilities.

\begin{itemize}
  \item The frontend parses and typechecks the source and converts it to core. The latter stage is called \textit{desugaring}
  \item The middle consists of a sequence of core-to-core transformations, responsible for most of the optimisation work.
  \item The back end translates the final core program into machine code, considering only the low level optimisations that the middle stage cannot.
\end{itemize}

The goal is that the bulk of the optimisation work is done in the middle through separately verifiable, compact, transformations. \cite{haskell_optimisations_1997}

\subsection{A Core Language}

As we already eluded to in the previous section, there exists something called the \textit{core} language (Which we will simply refer to as core from now on).
The core AST can be defined in all its brevity as follows:

\begin{listing}[H]
\begin{minted}{haskell}
data Expr
  | Var Id
  | Lit Literal
  | App Expr Expr
  | Lam Bndr Expr
  | Let Bind Expr
  | Case Expr Bndr Type [Alt]
  | Cast Expr CoercionR -- safe coercions are required for GADTs
  | Tick Tickish Expr  -- annotation that can contain some meta data
  | Type Type
  | Coercion Coercion
  
data Alt = Alt AltCon [Bndr] Expr
 
data Bind
  = NonRec Bndr Expr
  = Rec [(Bndr, Expr)]
  
-- the Id type contains information about
-- a variable, like it's name, a unique identifier
-- and analysis results
type Bndr = Id
\end{minted}
\caption{Slightly simplified definition of the core language.}
\label{code:core_def}
\end{listing}

Writing an optimisation transformation essentially of type \hs{[Bind] -> [Bind]} does not now seem
as daunting given the drastically reduced number of cases to consider. 
Last but not least, maintaining and debugging transformations is likewise a lighter task.


\subsubsection{A small note about \mono{Tree}}

It is important to mention that even though \mono{addFive} seems a little contrived,
this compositional style of programming is very persuasive in functional languages.
Hardly surprising, given how elegant and convenient it is.

In this example we cannot escape the need for defining rewrite rules since GHC has
decided to not be the enforcer of functor-laws. That is, it will not assume that
\mono{fmap f . fmap g = fmap (f. g)} because this could change the semantics of
functors that have decided to violate this law.

\section{Contemporary Haskell comprehension}

\subsection{Communicating in core}
\label{section:communicating_core}

We are not pioneers discovering the land of core inspection. Since its inception, core representation have
been used to communicate about programs and compiler interactions. Sifting through open issues on the
GHC compiler itself, one quickly comes across discussions elaborated by a core snippet. Consider issue
\href{https://gitlab.haskell.org/ghc/ghc/-/issues/22207}{\#22207} titled `\textit{bytestring Builder performance regressions after 9.2.3}' for example.
\hfill \break

\textit{While testing the performance characteristics of a bytestring patch meant to mitigate withForeignPtr-related performance regressions,
it was noticed that several of our Builder-related benchmarks have also regressed seriously for unrelated reasons.
The worst offender is byteStringHex, which on my machine runs about 10 times slower and allocates 21 times as much when using ghc-9.2.4 or ghc-9.4.2
as it did when using ghc-9.2.3. Here's a small program that can demonstrate this slowdown:}
\hfill \break

It then provides two snippets containing the final core representation of \mono{byteStringHex} as produced by the two different GHC version.
Each of these documents are around 400 lines of un-highlighted code with all available information. And while having all available information
sounds like a good thing (and it is in a way) it poses a serious practicality issue.
Namely: it is exceedingly difficult for a human to read and comprehend a certain aspect of the AST while having to filter out another.
Not to mention, it solidifies reading core as an activity reserved for only the most expert Haskell developers by, scaring others away with
a steep barrier to entry.

\subsection{Current tooling}
Efforts into making core inspection more approachable have been made. We discuss them here.

\subsubsection{Plain GHC telemetry}

Core snippets of your program can easily be coerced out of GHC. The most information you can get
is the core AST at each pass of the optimisation pipeline by using \mono{-ddump-core2core}.
To reduce the signal-to-noise ratio of core snippets, one can use any number of suppression options.
It is common to supress type arguments and type applications for example. These are largely
uninteresting because core is only typed to facilitate the correctness preserving checks that core-lint performs
and do not affect the compilation in any way.

As can be read in the GHC documentation, the following suppression flags are available to help to tame the beast.

\begin{table}[H]
  \begin{tabular}{p{0.35\linewidth}|p{0.65\linewidth}}
  \textbf{GHC flag} & \textbf{Effect on core printing} \\
  \hline
           & \\
  \mono{-dsuppress-all} & In dumps, suppress everything (except for uniques) that is suppressible.  \\
  \mono{-dsuppress-coercions} & Suppress the printing of coercions in Core dumps to make them shorter  \\
  \mono{-dsuppress-core-sizes} & Suppress the printing of core size stats per binding (since 9.4)  \\
  \mono{-dsuppress-idinfo} & Suppress extended information about identifiers where they are bound  \\
  \mono{-dsuppress-module-prefixes} & Suppress the printing of module qualification prefixes  \\
  \mono{-dsuppress-ticks} & Suppress "ticks" in the pretty-printer output.  \\
  \mono{-dsuppress-timestamps} & Suppress timestamps in dumps  \\
  \mono{-dsuppress-type-applications} & Suppress type applications  \\
  \mono{-dsuppress-type-signatures} & Suppress type signatures  \\
  \mono{-dsuppress-unfoldings} & Suppress the printing of the stable unfolding of a variable at its binding site  \\
  \mono{-dsuppress-uniques} & Suppress the printing of uniques in debug output (easier to use diff)  \\
  \mono{-dsuppress-var-kinds} & Suppress the printing of variable kinds\\
\end{tabular}
\end{table}

\subsubsection{GHC Plugins}

GHC --- being a playground for academics and enthusiasts alike --- is extremely flexible when it comes to
altering its functionality. Using the now well established plugin interface, one is able to hook into almost
any operation of the front- and midsection of the compiler. One such hook is managing the core2core passes that
will be run on the current module. This point of entry can be used to intersperse each core2core pass with an
identity transformation that smuggles away a copy of the AST in its full form.

One such existing plugin is \mono{ghc-dump} \cite{ghc_dump}. Besides extracting intermediate ASTs, it defines an
auxiliary core definition to which it provides a GHC version agnostic mapping. This has the increased benefit of 
being able to directly compare snapshots from different GHC versions; A not uncommon task as discussed in \cref{section:communicating_core}.
And while certainly being an improvement over plain text representation, we believe exploring and comparing such
dumps requires a more rich interface.

\subsection{Non-functional requirements: \mono{inspection-testing}}
\label{section:background_inspection_testing}

We know that producing the correct answer is usually not enough to make a program admissible,
we also care about runtime and memory requirements. This collection of constraints are examples of 
Non-functional requirements. In trying to meet these requirements, we expect an optimising compiler
to do some of the heavy lifting. Compositional style programming like in Haskell puts even more
faith in the compiler in this regard.

To illustrate, the very popular \mono{text} library makes the following promise: `\textit{Most of the functions in this module are subject to
fusion, meaning that a pipeline of such functions will usually allocate at most one Text value.}' \cite{inspection_testing}
Such promises cannot be checked with traditional tests as they do not concern the functionality of the code. As such,
they are considered to be \textit{Non-functional requirements}.

As identified by Breitner \cite{inspection_testing}, the aforementioned promise by the \mono{text} library had in fact
been broken in version \mono{1.2.3}, shown by the following counter example:

\begin{minted}{haskell}
import qualified Data.Text as T
import qualified Data.Text.Encoding as TE
import Data.ByteString

countChars :: ByteString -> Int
countChars = T.length . T.toUpper . TE.decodeUtf8
\end{minted}

Although \mono{countChars} uses a value of type \mono{Text} during the computations, they need not exist
considering that all operations can be done directly on the provided \mono{ByteString}. To fully comprehend
that statement we have to take a look at the definition \hs{data Text = Text ByteString Int Int}. This definition
helps us to see a \mono{Text} object is in fact just a \textit{view} into some bytestring by using an \hs{Int} for
an offset and length parameter. Because the call \mono{length} collapses this view back to a simple integer, we
should in theory be able to rewrite the definition without ever constructing this \mono{Text} object.
As mentioned however, this was not the case in practice due to a missing rewrite rule.

In itself this example formed the main motivation to develop a method to tests these non-functional requirements.
The result is the \mono{inspection-testing} package. It provides the machinery necessary to add the following statement
directly to the source file, preventing the same regression from occurring in the future.

\begin{minted}{haskell}
inspect $ 'countChars `hasNoType` ''T.Text
\end{minted}

Despite preemptively saving us from future regressions, the tests do not address the skill and fines required to identify
and patch the underlying issue. In this very example the final core will have undergone many in-linings producing
over 100 lines to painfully sift through without little more ergonomics than a string search. Moreover, we risk having
to repeat this work in the future when diagnosing what lies at the origin of any such test failing in the future.
